{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Set your own stopping conditions for pre-prunning\n",
    "'''\n",
    "Here, I would like to use running time as as stoppping condition\n",
    "To do so, I would keep track of the running time and stop the programme\n",
    "once it exceeds the maximum limit\n",
    "'''\n",
    "def stop_6(current_split, max_split):\n",
    "    \n",
    "    if current_split>=max_split:\n",
    "        print \"Stop Condition 6: Exceed maximum number of splits\"\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "#Add the the new stop condition to the tree\n",
    "\n",
    "def ClassificationTree(samples, output, features, step, tree_depth, max_depth, min_number, min_infogain, split, max_split):\n",
    "    '''This function is used to build a classification tree in a recursive way.\n",
    "       Remember how you build a binary tree in the previous Java and Data Structure courses).\n",
    "       \n",
    "    Inputs:\n",
    "    1) samples: Samples in the current tree node before making split on the feature (Pandas Dataframe)\n",
    "    2) output: Name of the output column\n",
    "    3) features: A list of feature names\n",
    "    4) step: The current binary split step\n",
    "    5) tree_depth: The depth of the current tree\n",
    "    6) max_depth: Maximum depth this tree can grow\n",
    "    7) min_number: Minimum number of node size\n",
    "    8) min_infogain: Minimum information gain\n",
    "    9ï¼‰split: current number of split\n",
    "    10) max_split: the maximum number of split\n",
    "    \n",
    "    Outputs:\n",
    "    1) tree_nodes: Nested tree nodes, which are stored and shown in nested dictionary type    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # If samples are empty, return None\n",
    "    if samples is None or len(samples)==0:\n",
    "        return None\n",
    "           \n",
    "    \n",
    "    current_features = features # Current feature list\n",
    "    labels = samples[output] # Output labels in the current tree node\n",
    "\n",
    "    print \"----------------------------------------------------------------------------\"\n",
    "    print \"----------------------------------------------------------------------------\"\n",
    "    print \"Step %s: Current tree depth is %s. Current tree node has %s data points\" % (step, tree_depth, len(samples)) # Sample size\n",
    "    \n",
    "    # Verify whether stopping conditions 1-4,6 are satisfied. If satisfied, return a leaf_node\n",
    "    if stop_1(labels) or stop_2(current_features) or stop_3(tree_depth, max_depth) or stop_4(samples, min_number) or stop_6(split, max_split):\n",
    "        return {\n",
    "                'label': majority_vote(labels),\n",
    "                'left_tree': None,\n",
    "                'right_tree': None,\n",
    "                'best_feature': None          \n",
    "            \n",
    "                }\n",
    "    \n",
    "    # If pass stopping conditions 1-4 and 6, then do best splitting\n",
    "    best_split = best_feature_split(samples, output, current_features)\n",
    "    best_feature, best_infogain, best_left, best_right = best_split[0], best_split[1], best_split[2], best_split[3]\n",
    "    \n",
    "    # Verify whether stopping condition 5 is satisfied. If satisfied, return a leaf node\n",
    "    if stop_5(best_infogain, min_infogain):\n",
    "        return {\n",
    "                'label': majority_vote(labels),\n",
    "                'left_tree': None,\n",
    "                'right_tree': None,\n",
    "                'best_feature': None          \n",
    "            \n",
    "                } \n",
    "    \n",
    "    # If pass stopping condition 5, then move on\n",
    "    step += 1\n",
    "    split += 1\n",
    "    \n",
    "    print \"Step %s: Binary split on %s. Size of Left and Right tree is (%s, %s)\" % \\\n",
    "          (step, best_feature, len(best_left) if best_left is not None else 0, len(best_right) if best_right is not None else 0)\n",
    "    current_features.remove(best_feature) # Remove this feature if this feature is used for split\n",
    "    \n",
    "    # Do binary split on left tree and right tree in a recursive way\n",
    "    left_split = ClassificationTree(best_left, output, current_features, step+1, tree_depth+1, max_depth, min_number, min_infogain, split, max_split) \n",
    "    right_split = ClassificationTree(best_right, output, current_features, step+1, tree_depth+1, max_depth, min_number, min_infogain, split, max_split) \n",
    "    \n",
    "    return {\n",
    "            'label': None,\n",
    "            'left_tree': left_split,\n",
    "            'right_tree': right_split,\n",
    "            'best_feature': best_feature        \n",
    "            \n",
    "            }  \n",
    "\n",
    "\n",
    "#Test the new build-tree function\n",
    "features = list(dataset.columns[2:])\n",
    "output = dataset.columns[1]\n",
    "#tree_model = ClassificationTree(dataset_copy, output, features, step=0, tree_depth=0, max_depth=7, min_number=5, min_infogain=5e-4, split = 0, max_split = 5)\n",
    "#tree_model\n",
    "dataset.head(n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Try to do post prunning\n",
    "'''\n",
    "  General Idea: \n",
    "  One common aproach of post-prunig aims to retain the decision tree but to replace some of its subtrees by leaf nodes,\n",
    "thus converting a complete tree to a smaller pruned one which predicts the classification of unseen instances at \n",
    "least as accurately.\n",
    "  we look for non-leaf nodes in the tree that has a descendant subtree of 1. Then we use prunning dataset to calculate\n",
    "the misclassification rate at each node. For every non-leaf nodes in the tree with descendant subtree of 1, we calculate\n",
    "and compare its misclassification rate with the weighted average of its child nodes. If it is smaller, we turn that node\n",
    "into a leaf. Continue this process recursively until there are no nodes to prun.\n",
    "\n",
    "'''\n",
    "\n",
    "# Before we wirte the code for post prunning, we modify the original decision tree function and add a couple of more functions\n",
    "\n",
    "#Modify the build tree function in the tutorial, so that it includenumber of instances in the training set\n",
    "#that belongs to the leaf\n",
    "def ClassificationTree(samples, output, features, step, tree_depth, max_depth, min_number, min_infogain):\n",
    "    '''This function is used to build a classification tree in a recursive way.\n",
    "       Remember how you build a binary tree in the previous C++ and Data Structure courses).\n",
    "       \n",
    "    Inputs:\n",
    "    1) samples: Samples in the current tree node before making split on the feature (Pandas Dataframe)\n",
    "    2) output: Name of the output column\n",
    "    3) features: A list of feature names\n",
    "    4) step: The current binary split step\n",
    "    5) tree_depth: The depth of the current tree\n",
    "    6) max_depth: Maximum depth this tree can grow\n",
    "    7) min_number: Minimum number of node size\n",
    "    8) min_infogain: Minimum information gain\n",
    "    Outputs:\n",
    "    1) tree_nodes: Nested tree nodes, which are stored and shown in nested dictionary type    \n",
    "    \n",
    "    '''\n",
    "    if sample is None or len(samples) == 0:\n",
    "        return None\n",
    "    \n",
    "    current_features = features # Current feature list\n",
    "    labels = samples[output] # Output labels in the current tree node\n",
    "\n",
    "    print \"----------------------------------------------------------------------------\"\n",
    "    print \"----------------------------------------------------------------------------\"\n",
    "    print \"Step %s: Current tree depth is %s. Current tree node has %s data points\" % (step, tree_depth,len(samples))\n",
    "    \n",
    "    # Verify whether stopping conditions 1-4 and 6 are satisfied. If satisfied, return a leaf_node\n",
    "    if stop_1(labels) or stop_2(current_features) or stop_3(tree_depth, max_depth) or stop_4(samples, min_number):\n",
    "        return {\n",
    "                'label': majority_vote(labels),\n",
    "                'left_tree': None,\n",
    "                'right_tree': None,\n",
    "                'best_feature': None,\n",
    "                'number': len(labels)\n",
    "                }\n",
    "    # If pass stopping conditions 1-4, then do best splitting\n",
    "    best_split = best_feature_split(samples, output, current_features)\n",
    "    best_feature, best_infogain, best_left, best_right = (best_split[0], best_split[1], best_split[2], best_split[3])\n",
    "    \n",
    "    # Verify whether stopping condition 5 is satisfied. If satisfied, return a leaf node\n",
    "    if stop_5(best_infogain, min_infogain):\n",
    "        return {\n",
    "                'label': majority_vote(labels),\n",
    "                'left_tree': None,\n",
    "                'right_tree': None,\n",
    "                'best_feature': None,\n",
    "                'number': len(labels)\n",
    "            \n",
    "                } \n",
    "    # If pass stopping condition 5, then move on\n",
    "    step += 1\n",
    "    print \"Step %s: Binary split on %s. Size of Left and Right tree is (%s, %s)\" % (step, best_feature, len(best_left), len(best_right))\n",
    "    current_features.remove(best_feature) # Remove this feature if this feature is used for split\n",
    "    \n",
    "    # Do binary split on left tree and right tree in a recursive way\n",
    "    left_split = ClassificationTree(best_left, output, current_features, step+1, tree_depth+1, max_depth, min_number, min_infogain)\n",
    "    right_split = ClassificationTree(best_right, output, current_features, step+1, tree_depth+1, max_depth, min_number, min_infogain)\n",
    "    \n",
    "    return {\n",
    "            'label': None,\n",
    "            'left_tree': left_split,\n",
    "            'right_tree': right_split,\n",
    "            'best_feature': best_feature,\n",
    "            'number': 0\n",
    "            \n",
    "            }  \n",
    "\n",
    "\n",
    "def majority_vote(output_labels):\n",
    "    \n",
    "    '''This function is used to get predicted label based on \"Majority Voting\" criterion for the current leaf node.     \n",
    "    Inputs:\n",
    "    1) output_labels: Outputs (labels) in this leaf node, such as [1, 0, 0, 1, 1]\n",
    "    \n",
    "    Outputs:\n",
    "    1) prediction: Predicted label for this leaf node (e.g., 0/-1, or 1)\n",
    "    \n",
    "    '''    \n",
    "    # numpy array\n",
    "    output_labels = np.array(output_labels)\n",
    "    \n",
    "    # Empty label\n",
    "    if output_labels.size == 0:\n",
    "        return None\n",
    "    \n",
    "    # Count output labels (0/-1 or 1)\n",
    "    values = np.unique(output_labels)\n",
    "    \n",
    "    if len(values) == 1:\n",
    "        return values[0]\n",
    "    else:\n",
    "        num0 = len(output_labels[output_labels == values[0]])\n",
    "        num1 = len(output_labels[output_labels == values[1]])\n",
    "        return values[1] if num1 >= num0 else values[0]  # Prediction based on \"Majority Voting\" criterion   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Post prunning Cotinued\n",
    "'''\n",
    "# Functon to find the deepest internal node\n",
    "\n",
    "def deepest_node(tree,path):\n",
    "    '''\n",
    "    inputs:\n",
    "    tree: the decision tree\n",
    "    path: an empty list to contain the path later\n",
    "    '''\n",
    "    if tree['label'] and path:  #reach a leaf\n",
    "        path.pop() #pop up the last move\n",
    "        return path;\n",
    "    \n",
    "    #make a deep copy of the original past list\n",
    "    left_path = list(path)\n",
    "    right_path = list(path)\n",
    "    \n",
    "    left_path.append('left')\n",
    "    right_path.append('right')\n",
    "    \n",
    "    #recursion\n",
    "    left = deepest_node(tree['left_tree'],left_path)\n",
    "    right = deepest_node(tree['right_tree'], right_path)\n",
    "    \n",
    "    #if the left tree is deeper\n",
    "    if len(left)>=len(right):\n",
    "        return left\n",
    "    else:\n",
    "        return right\n",
    "    \n",
    "    \n",
    "    \n",
    "#function to replace a node with a leaf by the majority of classification\n",
    "def replace_deepest_node(tree):\n",
    "    \n",
    "    feature = dict();\n",
    "    path = deepest_node(tree,[])\n",
    "    for i in path:\n",
    "        if i=='left':\n",
    "            feature = tree['left_tree']\n",
    "        else:\n",
    "            feature = tree['right_tree']\n",
    "    \n",
    "    #turn the internal node to a leaf\n",
    "    left = feature['left_tree']\n",
    "    right = feature['right_tree']\n",
    "    \n",
    "    #find the number of instances of each leaf\n",
    "    numberLeft = left['number']\n",
    "    numberRight = right['number']\n",
    "    \n",
    "    #eliminate the original child leaf\n",
    "    feature['left_tree'] = None\n",
    "    feature['right_tree'] = None\n",
    "    \n",
    "    feature['number'] = numberLeft + numRight\n",
    "    \n",
    "    feature['label'] = left['label'] if numberLeft>=numberRight else right['label']\n",
    "    \n",
    "    return \n",
    "    \n",
    "        \n",
    "#The final prunning function\n",
    "import copy\n",
    "\n",
    "def post_prunning(tree, test_data,features,output):\n",
    "    '''\n",
    "    input:\n",
    "    tree: the decision tree generated\n",
    "    test_data: test data to measure the performance of the tree\n",
    "    features: name of the features to be selected\n",
    "    output: the name of the output column\n",
    "    \n",
    "    outputs:\n",
    "    return the updated tree\n",
    "    '''\n",
    "    # if the root of tree is a leaf\n",
    "    if tree['label']:\n",
    "        return tree\n",
    "    \n",
    "    actual_labels = test_data[output]\n",
    "    error_rate = MissClassificationRate(tree,test_data[features],actual_labels)\n",
    "    \n",
    "    temp_tree = copy.deepcopy(tree)\n",
    "    \n",
    "    replace_deepest_node(tree)\n",
    "    \n",
    "    new_error_rate = MissClassificatonRate(tree,test_data[features],actual_labels)\n",
    "    \n",
    "    if(error_rate<new_error_rate): //the error rate increases when do the prunning\n",
    "        return temp_tree\n",
    "    else:\n",
    "        return post_prunning(tree,test_data,features,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Use Gini_index for binary splitting\n",
    "def Gini_index(sample_labels):\n",
    "\n",
    "    # sample is a list including numbers representing classification group.\n",
    "    # e.g. [0,0,1,1,1]\n",
    "    sample_labels = np.array(sample_labels)\n",
    "    if sample_labels.size == 0:\n",
    "        return 0\n",
    "    class_values = np.unique(sample_labels)\n",
    "    if class_values.size == 1:\n",
    "        return 0 \n",
    "    num1 = len(filter(lambda x: x == class_values[0],sample_labels))\n",
    "    num2 = len(filter(lambda x: x == class_values[1],sample_labels))\n",
    "    p1 = num1/(num1+num2)\n",
    "    p2 = 1 - p1\n",
    "    gini_index = 1-p1**2-p2**2\n",
    "    return gini_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Write a function to calculate missclassification rate\n",
    "def MissClassificationRate(train_tree,table,actual_labels):\n",
    "    predicted_label = []\n",
    "    \n",
    "    for i in range(table.shape(0)):\n",
    "        new_sample = table.iloc(i)  # extract the row of table\n",
    "        predicted_label.append(predict_label(new_sample,train_tree))\n",
    "    \n",
    "    if len(predicted_labels)!=len(actual_labels):\n",
    "        print \"Wrong prediction: number of elements mismatch\"\n",
    "        return 1;\n",
    "    \n",
    "    n = len(actual_labels)\n",
    "    wrong = 0\n",
    "    for i in range(n):\n",
    "        if(predicted_labels[i] != actual_labels[i]):\n",
    "            wrong+=1\n",
    "            \n",
    "    return wrong/n\n",
    "    \n",
    "\n",
    "#The predict_label function in the tutorial notes\n",
    "def predict_label(new_sample, train_tree):   \n",
    "    '''This function is used to predict the label of one new sample.\n",
    "    Inputs:\n",
    "    1) new_sample: A new sample, we would like to predict its label (Pandas DataFrame)\n",
    "    2) train_tree: The classification tree we have just trained\n",
    "    \n",
    "    Outputs:\n",
    "    1) predict_label: The predicted label for this new sample  \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # If move to the leaf node\n",
    "    if train_tree['best_feature']== None:\n",
    "        return train_tree['label']\n",
    "    # If still stay at temporary node\n",
    "    \n",
    "    else:\n",
    "        # Find the value of the best feature in the current node\n",
    "        # If value is 0, then go to left tree\n",
    "        # If value is 1, then go to right tree\n",
    "        # Remember what your have learned in Data Structure course, about binary tree\n",
    "        best_feature = train_tree['best_feature']\n",
    "        return predict_label(new_sample, train_tree['left_tree']) if new_sample[best_feature]==0 else predict_label(new_sample, train_tree['right_tree'])\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.1 What if features are continuous? Expain in words what would happen\n",
    "'''\n",
    "For features that are continous, we need to use threshold values to split the variable\n",
    "into several non-overlapping ranges and then treate it as a categorical variable. The cut\n",
    "points should reflect real properties of the domain.\n",
    "\n",
    "Methods to do this include \n",
    "A. local discretisation: \n",
    "    1. For each continuous attribute A\n",
    "     a) Sort the instances into ascending numerical order.\n",
    "     b) If there are n distinct values v1, v2, ..., vn, calculate the values of information gain \n",
    "     (or other measure) for each of the nâˆ’1 corresponding pseudo-attributes A < v2, A < v3, ..., A < vn.\n",
    "     c) Find which of the n âˆ’ 1 attribute values gives the largest value of information gain \n",
    "     (or optimises some other measure). If this is vi re- turn the pseudo-attribute A < vi, and the value of the \n",
    "     corresponding measure.\n",
    "    2. Calculate the value of information gain (or other measure) for any categor- ical attributes.\n",
    "    3. Select the attribute or pseudo-attribute with the largest value of informa- tion gain \n",
    "    (or which optimises some other measure).\n",
    "\n",
    "B. global discretisation: ChiMerge algorithm\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.2 What if output is continuous? Explain in words what would happen\n",
    "'''\n",
    "When output is continuous, still, we use threshold values to split the variable\n",
    "into several non-overlapping ranges and then treate it as a categorical variable.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
